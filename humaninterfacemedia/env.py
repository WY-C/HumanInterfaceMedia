#ëª¨ë¸ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import gymnasium as gym
import numpy as np
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld
from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from ray.rllib.algorithms.ppo import PPO
import os
import torch
import random
from ray.rllib.policy import Policy # ğŸ‘ˆ Policyë¥¼ import í•©ë‹ˆë‹¤.
import torch.nn.functional as F
from overcooked_ai_py.agents.agent import GreedyHumanModel
from overcooked_ai_py.planning.planners import (
    NO_COUNTERS_PARAMS,
    MediumLevelActionManager,
    MotionPlanner,
)


ACTION_MAP = {
    0: (0, -1),   # NORTH
    1: (0, 1),    # SOUTH
    2: (1, 0),    # EAST
    3: (-1, 0),   # WEST
    4: (0, 0),    # STAY
    5: "interact" # INTERACT
}

#for greedy human model
REVERSE_ACTION_MAP = {
    (0, -1): 0,   # NORTH
    (0, 1): 1,    # SOUTH
    (1, 0): 2,    # EAST
    (-1, 0): 3,   # WEST
    (0, 0): 4,    # STAY
    "interact": 5 # INTERACT
}


class Rllib_multi_agent(MultiAgentEnv):
    #agent1, agent2
    def __init__(self, config = None, reward_shaping = True):
        #ì´í›„ configì— layout name, horizon ë“±ë“±ì„ ë„£ì–´ì•¼í•¨.
        super().__init__()
        config = config or {}
        layout_name = config.get("layout_name", "cramped_room1")
        horizon = config.get("horizon", 1000000)
        self.reward_shaping = config.get("reward_shaping", True)
        mdp = OvercookedGridworld.from_layout_name(layout_name) 
        self.overcooked_env = OvercookedEnv.from_mdp(mdp, horizon=horizon, info_level=0)
        self.count_delivery_soup = 0
        self.previous_trajectory = [0] * 401

        self.agents = ["agent_0", "agent_1"]
        self._agent_ids = {"agent_0", "agent_1"}
        #self._agent_ids = set(self.agents)
        sample_obs_dict = self._get_obs(0)
        flattened_shape = sample_obs_dict['agent_0'].flatten().shape
        self.observation_space = gym.spaces.Dict(
            {
                "agent_0": gym.spaces.Box(low=0, high=1, shape=flattened_shape, dtype=np.float32),
                "agent_1": gym.spaces.Box(low=0, high=1, shape=flattened_shape, dtype=np.float32),
            }
        )
        self.action_space = gym.spaces.Dict(
            {
                "agent_0": gym.spaces.Discrete(len(ACTION_MAP)),
                "agent_1": gym.spaces.Discrete(len(ACTION_MAP)),
            }
        )
        
        # self.observation_space = gym.spaces.Box(
        #     low=0, high=1, shape=sample_obs['agent_1'].shape, dtype=np.int32
        # )
        
        # self.action_space = gym.spaces.Discrete(len(ACTION_MAP))
        #print(sample_obs.shape)


    def _get_obs(self, idx = 0):
        state = self.overcooked_env.state

        #print(state.shape)
        obs_tuple = self.overcooked_env.lossless_state_encoding_mdp(state)
        observations = {
            self.agents[0]: obs_tuple[0].flatten().astype(np.float32),
            self.agents[1]: obs_tuple[1].flatten().astype(np.float32),
        }
        #obs1 = np.array(obs).flatten()
        return observations

    #obs, reward ê³µìœ ë¨.
    def reset(self, seed=None, options=None):
        #trajectory ì €ì¥ìš© ë³€ìˆ˜
        self.trajectory = []
        self.timestep = 0
        self.count_delivery_soup = 0
        # print(self.previous_trajectory)

        """í™˜ê²½ì„ ë¦¬ì…‹í•˜ê³  ê° ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ê´€ì¸¡ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        self.overcooked_env.reset()
        #self.agents = ["agent_1", "agent_2"]
        # ê° ì—ì´ì „íŠ¸ IDì— ëŒ€í•œ ê´€ì¸¡ê°’ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        obs = self._get_obs()
        return obs, {}

    def step(self, action_dict):

        #print("Received action_dict:", action_dict)
        # if action_dict == {}:
        #     action_dict['agent_1'] = 4
        #     action_dict['agent_2'] = 4
        #     print(1)
        #print(action_dict)
        actions = [ACTION_MAP[action_dict[agent_id]] for agent_id in self.agents]

        next_state, rewards, done, info = self.overcooked_env.step(actions)
        obs = self._get_obs()
        shaped_rewards_list = info["shaped_r_by_agent"]

        if rewards > 20:
            print(self.count_delivery_soup)

            self.count_delivery_soup += 1
        

        if self.reward_shaping == True:
            reward = {
            self.agents[0]: rewards + shaped_rewards_list[0],
            self.agents[1]: rewards + shaped_rewards_list[1],         
            }
        else:
            reward = {
            self.agents[0]: rewards,
            self.agents[1]: rewards,         
        }

        done_dict = {
            self.agents[0]: done,
            self.agents[1]: done,  
        }
        truncated_dict = {
            self.agents[0]: False,
            self.agents[1]: False,  
        }
        done_dict["__all__"] = done
        truncated_dict["__all__"] = False
        info_dict = {
            self.agents[0]: info,
            self.agents[1]: info, 
        }
        self.trajectory.append(repr(self.overcooked_env))
        self.timestep +=1
        if done_dict["__all__"]:
             self.previous_trajectory = self.trajectory.copy()
             #print(self.previous_trajectory)
        return obs, reward, done_dict, truncated_dict, info_dict

    
    def render(self, mode="rgb-array"):
        print(self.overcooked_env)

def set_partner(path):
    checkpoint_path = os.path.abspath(path)
    restored_trainer = PPO.from_checkpoint(checkpoint_path)
    module = restored_trainer.get_module("shared_policy")
    return module

def get_partner_action(module, obs):
    agent_ids = sorted(obs.keys())
    obs_list = [obs[agent_id] for agent_id in agent_ids]
    
    # RLModuleì„ ì‚¬ìš©í•´ í–‰ë™ ì¶”ë¡ 
    module_input = {
        "obs": torch.from_numpy(np.stack(obs_list))
    }
    action_tensors = module.forward_inference(module_input)
    
    # ë¡œì§“(logits)ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í–‰ë™ì„ ì„ íƒ (argmax)
    logits = action_tensors['action_dist_inputs']
    probs = F.softmax(logits, dim=1)

    actions_tensor = torch.multinomial(probs, num_samples=1).squeeze(1)
    actions_np = actions_tensor.numpy()

    # ì¶”ë¡  ê²°ê³¼ë¥¼ action_dict í˜•íƒœë¡œ ë³€í™˜
    action_dict = {agent_id: action for agent_id, action in zip(agent_ids, actions_np)}
    return action_dict

class FCP_Rllib_for_visualization(gym.Env):
    #í•™ìŠµí•  ëª¨ë¸ì´ 0ë²ˆ, í•™ìŠµëœ ëª¨ë¸ì€ 1ë²ˆ
    def __init__(self, env_config=None):
        super().__init__()
        # â­ï¸â­ï¸â­ï¸ ë””ë²„ê¹…ì„ ìœ„í•œ í•µì‹¬ print ë¬¸ â­ï¸â­ï¸â­ï¸
        # RLlibìœ¼ë¡œë¶€í„° ë°›ì€ env_configë¥¼ ë‚´ë¶€ multi-agent í™˜ê²½ì— ì „ë‹¬í•©ë‹ˆë‹¤.
        self.multi_agent_env = Rllib_multi_agent(env_config)
        #self.horizon = env_config.get("horizon", 400)
        self.active_agent_id = "agent_0"
        self.partner_agent_id = "agent_1"
        # íŒŒíŠ¸ë„ˆ ëª¨ë¸ ë¡œë“œëŠ” í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        self.partner_paths ={
            1: "FCP_partner_agent/reward_595_24",
            }
        # ğŸ’¡ [ê°œì„  2] __init__ì—ì„œ ëª¨ë“  íŒŒíŠ¸ë„ˆ ëª¨ë“ˆì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
        self.partner_modules = {}
        print("="*30)
        print("íŒŒíŠ¸ë„ˆ ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        print("ëª¨ë“  íŒŒíŠ¸ë„ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print("="*30)
        self.observation_space = self.multi_agent_env.observation_space[self.active_agent_id]
        self.action_space = self.multi_agent_env.action_space[self.active_agent_id]
        self.total_reward = 0
        #random, greedy, mid, proê°€ ë½‘íŒ íšŸìˆ˜
        self.count = [0] * 4
        self.iteration = 0
        

        self.num_of_dish = 0
        
        self.episode_reward = 0


    def _get_obs(self, idx = 0):
        return self.multi_agent_env._get_obs()
    #obs, reward ê³µìœ ë¨.
    def reset(self, seed=None, options=None):
        obs_dict, info_dict = self.multi_agent_env.reset()
        return obs_dict[self.active_agent_id], {}


    def step(self, action):

        if isinstance(action, dict):
            action_dict_to_step = action
        else:
            action_dict_to_step = {
                self.active_agent_id: action,
                self.partner_agent_id: 2,
            }
        #print(action_dict_to_step)
        #print(action_dict_to_step)
        obs_dict, reward_dict, done_dict, trunc_dict, info_dict = self.multi_agent_env.step(action_dict_to_step)
        #print(info_dict)
        # 4. ë‹¨ì¼ ì—ì´ì „íŠ¸ í™˜ê²½ì˜ ê²°ê³¼ í˜•ì‹ì— ë§ê²Œ ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        observation = obs_dict[self.active_agent_id]
        reward = reward_dict[self.active_agent_id]
        #print(reward)
        if reward > 0:
            print(self.num_of_dish)

            self.num_of_dish += 1
        #print(self.multi_agent_env.overcooked_env.mdp.state_string(self.multi_agent_env.overcooked_env.state))
        

        self.total_reward += reward
        terminated = done_dict["__all__"] # ì—í”¼ì†Œë“œ ì„±ê³µ/ì‹¤íŒ¨ ì¢…ë£Œ ì—¬ë¶€
        truncated = trunc_dict["__all__"] # ì‹œê°„ ì´ˆê³¼ ì¢…ë£Œ ì—¬ë¶€
        # â­ï¸â­ï¸â­ï¸ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ â­ï¸â­ï¸â­ï¸
        if terminated or truncated:
            self.iteration +=1
        
        info = info_dict.get(self.active_agent_id, {})


        return observation, reward, terminated, truncated, info
    
    def get_num_of_dish(self):
        return self.num_of_dish


    def update_agent_pool(self, num):
        self.random_num = num
